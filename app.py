# -*- coding: utf-8 -*-
"""V6 (Stable version).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16X0635SD92L9txZedFdB59iQ-tNngHaf
"""

#Below installs required packages
!pip install faiss-cpu langchain-huggingface langchain-groq

"""#  TRY 2"""

import subprocess
import sys
import gradio as gr  # For creating the interactive web-based UI
import json
import datetime
import os
import random
import time
from typing import List, Dict, Tuple, Optional  # For type hints to improve code clarity
import pandas as pd
import nest_asyncio  # To allow nested asyncio event loops in Colab
nest_asyncio.apply()  # Apply patch to enable async in Colab's Jupyter environment

# FAISS and LangChain imports with error handling
try:
    import faiss  # Library for efficient similarity search
    from langchain_core.documents import Document  # LangChain's document representation
    from langchain_core.retrievers import BaseRetriever  # Base class for custom retrievers
    from langchain_huggingface import HuggingFaceEmbeddings  # For embedding text with HuggingFace models
    FAISS_AVAILABLE = True
except ImportError as e:
    FAISS_AVAILABLE = False
    print(f"FAISS import error: {str(e)}")  # Log if FAISS or dependencies are missing

# LLM API imports with error handling to support multiple providers
try:
    import openai  # OpenAI API client
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

try:
    import anthropic  # Anthropic API client
    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False

try:
    import google.generativeai as genai  # Google Gemini API client
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False

try:
    from langchain_groq import ChatGroq  # LangChain wrapper for Groq API
    GROQ_AVAILABLE = True
except ImportError:
    GROQ_AVAILABLE = False

import pickle  # For serializing FAISS texts
import numpy as np  # For handling embeddings as arrays
import re  # For parsing <think> blocks

# Custom FAISS retriever class for LangChain integration
class FAISSRetriever(BaseRetriever):
    """Custom FAISS retriever for LangChain to search AR6 Synthesis Report chunks."""
    def __init__(self, index_path: str, texts_path: str, embed_model: HuggingFaceEmbeddings, k: int = 3):
        """Initialize retriever with FAISS index, texts, and embedding model.

        Args:
            index_path: Path to FAISS index file (faiss_index.bin).
            texts_path: Path to pickled text chunks (faiss_texts.pkl).
            embed_model: HuggingFace embedding model for query encoding.
            k: Number of top documents to retrieve (default: 3).
        """
        super().__init__()
        self._embed_model = embed_model
        self._index = faiss.read_index(index_path)  # Load pre-built FAISS index
        with open(texts_path, 'rb') as f:
            self._texts = pickle.load(f)  # Load text chunks
        self._k = k

    def _get_relevant_documents(self, query: str) -> List[Document]:
        """Retrieve top-k relevant documents for a given query.

        Args:
            query: User input query string.

        Returns:
            List of Document objects containing relevant text chunks.
        """
        # Embed the query using the same model used for indexing
        query_embedding = self._embed_model.embed_query(query)
        query_embedding = np.array([query_embedding], dtype=np.float32)  # Convert to FAISS-compatible format
        distances, indices = self._index.search(query_embedding, self._k)  # Search FAISS index
        # Return Document objects for the top-k text chunks
        return [Document(page_content=self._texts[i]) for i in indices[0]]

# Main agent class for handling IPCC report queries
class IPCCLLMAgent:
    """IPCC Climate Reports LLM Agent for Google Colab with FAISS Integration.

    Manages LLM interactions, FAISS retrieval, and knowledge base for IPCC reports.
    """
    def __init__(self):
        """Initialize the agent with report metadata, LLM models, and FAISS retriever."""
        self.conversation_history = []  # Store chat history (not currently used)
        # Define IPCC report options for the UI dropdown
        self.ipcc_reports = {
            'all': {'name': 'All IPCC Reports', 'color': 'üåç'},
            'ar6_syr': {'name': 'AR6 Synthesis Report (2023)', 'color': 'üìñ'},
            'ar6': {'name': 'AR6 (2021-2023)', 'color': 'üå±'},
            'ar5': {'name': 'AR5 (2013-2014)', 'color': 'üìä'},
            'special': {'name': 'Special Reports', 'color': '‚ö†Ô∏è'},
            'ar4': {'name': 'AR4 (2007)', 'color': 'üìÑ'}
        }
        print("IPCC Reports initialized:", self.ipcc_reports.keys())  # Debug: confirm report keys
        # Define available LLM models for the UI dropdown
        self.llm_models = {
            'deepseek': {'name': 'DeepSeek-R1-Distill-Llama-70B', 'provider': 'Groq'},
            'llama': {'name': 'Llama-3.3-70B-Versatile', 'provider': 'Groq'},
            'gpt-4': {'name': 'GPT-4 Turbo', 'provider': 'OpenAI'},
            'gpt-3.5': {'name': 'GPT-3.5 Turbo', 'provider': 'OpenAI'},
            'claude-3': {'name': 'Claude 3 Sonnet', 'provider': 'Anthropic'},
            'gemini': {'name': 'Gemini Pro', 'provider': 'Google'},
            'mock': {'name': 'Mock AI (Demo)', 'provider': 'Local'}
        }
        self.setup_api_clients()  # Initialize LLM API clients
        self.ipcc_knowledge = self.load_ipcc_knowledge()  # Load static knowledge base
        self.faiss_retriever = self.setup_faiss_retriever()  # Setup FAISS for AR6 Synthesis

    def setup_api_clients(self):
        """Setup API clients with keys from environment or Colab Secrets."""
        # Initialize client attributes to None
        self.openai_client = None
        self.anthropic_client = None
        self.gemini_client = None
        self.groq_client_llama = None
        self.groq_client_deepseek = None

        from google.colab import userdata  # For accessing Colab Secrets

        # Retrieve API keys from environment or Colab Secrets
        openai_key = os.getenv('OPENAI_API_KEY')
        anthropic_key = os.getenv('ANTHROPIC_API_KEY')
        gemini_key = os.getenv('GEMINI_API_KEY')
        groq_key = os.getenv('LLAMA_API_KEY')
        groq_api_key = userdata.get('groqAPIkey2')  # Preferred key for Groq models

        print(f"GROQ_AVAILABLE: {GROQ_AVAILABLE}")  # Debug: confirm Groq library status
        print(f"FAISS_AVAILABLE: {FAISS_AVAILABLE}")  # Debug: confirm FAISS library status

        # Initialize OpenAI client if key and library are available
        if openai_key and OPENAI_AVAILABLE:
            self.openai_client = openai.OpenAI(api_key=openai_key)

        # Initialize Anthropic client if key and library are available
        if anthropic_key and ANTHROPIC_AVAILABLE:
            self.anthropic_client = anthropic.Anthropic(api_key=anthropic_key)

        # Initialize Google Gemini client if key and library are available
        if gemini_key and GEMINI_AVAILABLE:
            genai.configure(api_key=gemini_key)
            self.gemini_client = genai.GenerativeModel('gemini-pro')

        # Initialize Groq clients for DeepSeek and Llama models
        api_key = groq_key or groq_api_key  # Use either key
        if api_key and GROQ_AVAILABLE:
            try:
                self.groq_client_llama = ChatGroq(api_key=api_key, model_name='llama-3.3-70b-versatile')
                # Debug DeepSeek initialization
                try:
                    self.groq_client_deepseek = ChatGroq(api_key=api_key, model_name='deepseek-r1-distill-llama-70b')
                    print("DeepSeek initialized with primary model: deepseek-r1-distill-llama-70b")
                except Exception as e:
                    print(f"DeepSeek primary model failed: {str(e)}. Trying fallback model.")
                    self.groq_client_deepseek = ChatGroq(api_key=api_key, model_name='deepseek-r1')  # Fallback
                    print("DeepSeek initialized with fallback model: deepseek-r1")
                print("Groq clients initialized successfully")
            except Exception as e:
                print(f"Error initializing Groq clients: {str(e)}")
                self.groq_client_llama = None
                self.groq_client_deepseek = None
        else:
            print("No Groq API key found or langchain-groq not installed")

    def setup_faiss_retriever(self):
        """Setup custom FAISS retriever for AR6 Synthesis Report (2023).

        Loads pre-built FAISS index and text chunks from Google Drive.
        """
        if not FAISS_AVAILABLE:
            print("FAISS or required libraries not available. Skipping FAISS setup.")
            return None

        # Paths to FAISS index and text chunks on Google Drive
        faiss_path = "/content/drive/Shareddrives/Prof Cecil's projects/The LLM/faiss_index.bin"
        texts_path = "/content/drive/Shareddrives/Prof Cecil's projects/The LLM/faiss_texts.pkl"

        # Debug: verify file existence
        print("Checking FAISS file paths:")
        print(f"FAISS file exists: {os.path.exists(faiss_path)}")
        print(f"Texts file exists: {os.path.exists(texts_path)}")

        # Check if both files exist
        if not (os.path.exists(faiss_path) and os.path.exists(texts_path)):
            print("Warning: FAISS index or texts file not found. FAISS retriever cannot be loaded.")
            return None

        try:
            # Initialize embedding model for query encoding
            embed_model = HuggingFaceEmbeddings(model_name="mixedbread-ai/mxbai-embed-large-v1")
            print("Embedding model initialized")
            # Create FAISS retriever instance
            retriever = FAISSRetriever(faiss_path, texts_path, embed_model, k=3)
            print("Custom FAISS retriever loaded successfully")
            return retriever
        except Exception as e:
            print(f"Error setting up FAISS retriever: {str(e)}")
            return None

    def load_ipcc_knowledge(self) -> Dict:
        """Load static IPCC knowledge base with pre-summarized climate information.

        Returns:
            Dictionary mapping topic keys to content and sources.
        """
        return {
            'ar6_summary': {
                'content': """# üå°Ô∏è AR6 Synthesis Report: Key Findings for Policymakers
## **Physical Science Basis (Working Group I)**
- **Global Temperature Rise**: 1.1¬∞C above 1850-1900 levels
- **Human Influence**: Unequivocally the dominant cause of warming
- **Rate of Change**: Faster than any period in over 2,000 years
- **Regional Impacts**: Every inhabited region experiencing climate change
- **Irreversible Changes**: Many changes locked in for centuries to millennia

## **Impacts, Adaptation & Vulnerability (Working Group II)**
- **Population at Risk**: 3.3-3.6 billion people highly vulnerable
- **Current Impacts**: Widespread losses and damages already occurring
- **Food Security**: 828 million people undernourished (2021)
- **Water Stress**: Up to 3 billion people experience water scarcity
- **Ecosystem Degradation**: Widespread species shifts and ecosystem changes

## **Mitigation of Climate Change (Working Group III)**
- **Emission Trends**: Global GHG emissions continued to rise
- **Peak Requirement**: Emissions must peak before 2025 for 1.5¬∞C
- **2030 Target**: 43% reduction needed by 2030 (2019 levels)
- **2050 Target**: Net zero CO‚ÇÇ emissions required
- **Investment Gap**: $4 trillion annually needed in clean energy

## **Integrated Solutions**
- **Rapid Transformation**: Deep, immediate cuts across all sectors
- **Technology Readiness**: Many solutions available and cost-effective
- **Co-benefits**: Climate action improves health, economy, equity
- **Just Transitions**: Equitable pathways essential for success""",
                'sources': ['AR6 Synthesis Report SPM', 'AR6 WG1-3 Reports']
            },
            'urgent_actions_2030': {
                'content': """# üö® Critical Climate Actions Needed by 2030
## **Energy System Transformation**
### Renewable Energy Scale-up
- **Target**: 60% renewable electricity globally (vs ~30% today)
- **Solar**: Increase capacity 4x from 2020 levels
- **Wind**: Triple offshore wind capacity
- **Storage**: Deploy 120 GW of battery storage annually
### Fossil Fuel Phase-out
- **Coal**: Retire 2,400+ coal plants globally
- **Oil & Gas**: Reduce production 75% by 2050
- **Subsidies**: End $5.9 trillion in fossil fuel subsidies
## **Transport Decarbonization**
### Electric Vehicle Revolution
- **Target**: 50% of new car sales electric by 2030
- **Infrastructure**: 40 million public charging points needed
- **Heavy Transport**: 30% of trucks electric/hydrogen by 2030
### Sustainable Aviation & Shipping
- **Aviation**: 10% sustainable fuels by 2030
- **Shipping**: 5% zero-emission fuels by 2030
## **Buildings & Cities**
### Zero-Carbon Buildings
- **New Buildings**: All new buildings zero-carbon by 2030
- **Retrofits**: Deep renovation of 3% of building stock annually
- **Heat Pumps**: 600 million heat pumps by 2030
### Urban Planning
- **15-Minute Cities**: Reduce transport demand 20%
- **Green Infrastructure**: 30% urban tree canopy coverage
## **Natural Climate Solutions**
### Forest Protection
- **Deforestation**: End deforestation by 2030
- **Restoration**: 350 million hectares by 2030
- **Carbon Storage**: 5.8 GtCO‚ÇÇ annually from forests
### Sustainable Agriculture
- **Regenerative Practices**: 30% of farmland by 2030
- **Food Waste**: Reduce food waste 50%
- **Diets**: 20% shift toward plant-based diets
## **Financial Requirements**
- **Total Investment**: $4-6 trillion annually
- **Clean Energy**: $1.6-3.8 trillion annually
- **Nature**: $350 billion annually
- **Adaptation**: $140-300 billion annually by 2030""",
                'sources': ['AR6 WG3 Ch5', '1.5¬∞C Special Report', 'AR6 Synthesis']
            },
            'carbon_budgets': {
                'content': """# üéØ Carbon Budgets: The Climate Math Explained
## **What is a Carbon Budget?**
A carbon budget is the maximum amount of CO‚ÇÇ that can be emitted to limit global warming to a specific temperature target with a given probability.
## **Current Carbon Budget Status (AR6 Update)**
### For 1.5¬∞C Target (50% probability)
- **Remaining Budget**: ~400 GtCO‚ÇÇ from 2020
- **At Current Rate**: ~10 years remaining (40 GtCO‚ÇÇ/year)
- **Updated from AR5**: Previously ~1,000 GtCO‚ÇÇ (methodology improved)
### For 2¬∞C Target (67% probability)
- **Remaining Budget**: ~1,150 GtCO‚ÇÇ from 2020
- **At Current Rate**: ~29 years remaining
## **Key Insights**
### Why Budgets Matter
- **Linear Relationship**: Cumulative CO‚ÇÇ determines peak warming
- **Location Independent**: Doesn't matter where emissions occur
- **Timing Flexible**: When matters less than total amount
### Budget Uncertainties
- **Non-CO‚ÇÇ GHGs**: Methane, N‚ÇÇO add ~0.4¬∞C warming
- **Climate Feedbacks**: Could reduce budget by 100-200 GtCO‚ÇÇ
- **Temperature Response**: ¬±0.2¬∞C uncertainty in climate sensitivity
## **Sharing the Carbon Budget**
### Equity Principles
- **Historical Responsibility**: Developed countries 79% of budget since 1850
- **Capability**: High-income countries have greater resources
- **Development Needs**: Developing countries need emissions for basic needs
### Fair Share Approaches
- **Per Capita**: Equal emissions rights per person
- **Capability**: Based on ability to pay
- **Grandfathered**: Continue current emission shares.
- **Hybrid**: Adjust based on historical responsibility and current capability.
## **Policy Implications**
### Carbon Pricing
- **Target**: Carbon pricing to reach $135-$550 per ton by 2030 to align with 1.5¬∞C
- **Impact**: Encourages low-carbon technologies and behaviors
- **Revenue Use**: Funds for adaptation, mitigation, and just transitions
### National Pledges
- **NDCs**: Nationally Determined Contributions must align with global budgets
- **Gaps**: Current pledges exceed 1.5¬∞C budget by 20-30 GtCO‚ÇÇ
- **Updates**: Countries urged to enhance ambition every 5 years""",
                'sources': ["IPCC AR6", "Carbon Budgets 2023"]
            },
            'climate_impacts_risks': {
                'content': """# üåä Climate Impacts and Risks: What We Face
## **Observed Impacts (Already Happening)**
### Temperature Extremes
- **Heat Waves**: Frequency and intensity increased, e.g., 2021 Pacific Northwest heatwave
- **Hot Days**: Number of days above 35¬∞C doubled in some regions
- **Cold Snaps**: Becoming less frequent but still impactful
### Water Cycle Changes
- **Flooding**: Increased due to intensified rainfall and storms
- **Droughts**: More frequent and prolonged, affecting agriculture and water supply
- **Sea Level Rise**: 3.7 mm/year, accelerating due to melting glaciers and ice sheets
### Ecosystems
- **Biodiversity Loss**: Coral bleaching, species migration, habitat degradation
- **Wildfires**: Increased in intensity and frequency, e.g., Australian bushfires 2019-2020
## **Projected Risks (2030‚Äì2050)**
### Human Systems
- **Health**: Heat-related mortality up by 10-20% in vulnerable regions
- **Food Security**: Crop yields down 5-20% in tropical regions
- **Displacement**: 50-200 million climate refugees by 2050
### Natural Systems
- **Ocean Acidification**: Threatens marine ecosystems
- **Tundra Thaw**: Permafrost melt releases methane
- **Adaptation Needs**
  - **Infrastructure**: Coastal defenses, water management
  - **Policy**: Early warning systems, climate-resilient agriculture""",
                'sources': ["IPCC AR6", "WMO State of Climate 2023"]
            }
        }

    def format_response(self, content: str, sources: List[str] = None, report_focus: str = 'all') -> str:
        """Format LLM response with report focus and source citations.

        Args:
            content: Raw response text from LLM or knowledge base.
            sources: List of source references (e.g., IPCC report sections).
            report_focus: Selected report key (e.g., 'ar6_syr').

        Returns:
            Formatted response string with markdown styling.
        """
        formatted = f"**Report Focus**: {self.ipcc_reports[report_focus]['name']}\n\n{content}"
        if sources:
            formatted += f"\n\n**Sources**: {', '.join(sources)}"
        return formatted

    def get_mock_response(self, message: str, report_focus: str) -> Tuple[str, List[str]]:
        """Generate mock responses for demo mode when no LLM is available.

        Args:
            message: User query string.
            report_focus: Selected report key.

        Returns:
            Tuple of (response content, list of sources).
        """
        message_lower = message.lower()

        # Handle AR6 Synthesis Report queries in mock mode
        if report_focus == 'ar6_syr':
            return """This is a placeholder response for the AR6 Synthesis Report. The actual response would use FAISS-retrieved data when using a supported LLM. Please select a Groq-hosted model (DeepSeek or Llama) to access the FAISS data for the AR6 Synthesis Report (2023).""", ['Mock Response']

        # Match query to predefined knowledge base topics
        if any(word in message_lower for word in ['ar6', 'synthesis', 'key findings']):
            knowledge = self.ipcc_knowledge['ar6_summary']
            return knowledge['content'], knowledge['sources']

        elif any(word in message_lower for word in ['urgent', '2030', 'actions', 'immediate']):
            knowledge = self.ipcc_knowledge['urgent_actions_2030']
            return knowledge['content'], knowledge['sources']

        elif any(word in message_lower for word in ['carbon budget', 'budget', 'emissions budget']):
            knowledge = self.ipcc_knowledge['carbon_budgets']
            return knowledge['content'], knowledge['sources']

        elif any(word in message_lower for word in ['impacts', 'risks', 'effects', 'consequences']):
            knowledge = self.ipcc_knowledge['climate_impacts_risks']
            return knowledge['content'], knowledge['sources']

        else:
            return """I can help you with IPCC climate reports! Try asking about:
- AR6 Synthesis Report key findings
- Urgent climate actions needed by 2030
- Carbon budgets and emissions pathways
- Climate impacts and risks
- Comparison of AR5 and AR6 projections""", ['IPCC Knowledge Base']

    async def clean_response(self, content: str) -> str:
        """Clean DeepSeek response by removing <think> blocks and unwanted tags.

        Args:
            content: Raw response text from DeepSeek.

        Returns:
            Cleaned response string.
        """
        # Remove <think> and </think> tags
        cleaned = re.sub(r'<think>.*?</think>', '', content, flags=re.DOTALL)
        # Remove any remaining HTML-like tags
        cleaned = re.sub(r'<[^>]+>', '', cleaned)
        # Strip leading/trailing whitespace
        cleaned = cleaned.strip()
        return cleaned

    async def call_llm_api(self, messages: List[Dict], model: str, temperature: float, max_tokens: int, report_focus: str) -> Tuple[str, List[str]]:
        """Call appropriate LLM API with FAISS integration for AR6 Synthesis Report.

        Args:
            messages: List of chat history dictionaries [{"role": "user" | "assistant", "content": str}].
            model: Selected LLM model key (e.g., 'llama', 'deepseek').
            temperature: Response creativity (0.0 to 1.0).
            max_tokens: Maximum response length.
            report_focus: Selected report key.

        Returns:
            Tuple of (response content, list of sources).
        """
        if model == 'mock':
            time.sleep(1)  # Simulate API delay
            user_message = messages[-1]['content']
            return self.get_mock_response(user_message, report_focus)

        # Define system prompt for Llama and other models
        system_prompt = """You are an expert IPCC climate reports analyst. Provide accurate, science-based responses using information from IPCC Assessment Reports (AR4, AR5, AR6) and Special Reports.

Key points to note:
- Base responses on IPCC findings and scientific consensus
- Include specific data, figures, and projections when relevant
- Distinguish between different confidence levels and scenarios
- Cite specific IPCC reports and sections when possible
- Explain complex concepts clearly for policymakers and general audiences
- Highlight policy implications and actionable insights
- Note uncertainties and ranges in projections"""

        # Simplified prompt for DeepSeek, used as system message
        deepseek_prompt = """You are an expert on IPCC climate reports. Answer the query accurately using information from IPCC Assessment Reports (AR4, AR5, AR6) and Special Reports. Provide clear, science-based responses using specific data, figures, and projections where relevant. Cite IPCC reports and sections when possible, and explain concepts clearly for policymakers and general audiences."""

        # Handle AR6 Synthesis Report with FAISS retrieval for Groq models
        if report_focus == 'ar6_syr' and self.faiss_retriever and model in ['deepseek', 'llama']:
            user_message = messages[-1]['content']
            try:
                # Retrieve relevant chunks from FAISS
                docs = self.faiss_retriever._get_relevant_documents(user_message)
                context = "\n".join([doc.page_content for doc in docs])
                # Prepare messages for ChatGroq
                if model == 'deepseek':
                    prompt_messages = [
                        {"role": "system", "content": f"{deepseek_prompt}\n\nUse the following context from AR6 Synthesis Report (2023):\n{context}"},
                        {"role": "user", "content": user_message}
                    ]
                else:
                    prompt_messages = [
                        {"role": "system", "content": f"{system_prompt}\n\nUse the following context from AR6 Synthesis Report (2023):\n{context}"},
                        {"role": "user", "content": user_message}
                    ]

                print(f"Debug: Prompt messages for {model}: {prompt_messages}")  # Debug: Log prompt

                if model == 'deepseek' and self.groq_client_deepseek:
                    try:
                        response = self.groq_client_deepseek.invoke(prompt_messages)
                        print(f"Debug: Raw response for DeepSeek: {response}")  # Debug: raw response
                        content = await self.clean_response(response.content if hasattr(response, 'content') else str(response))
                        if not content or not content.strip():
                            print("DeepSeek returned empty or invalid content after cleaning. Falling back to mock response.")
                            return self.get_mock_response(user_message, report_focus)
                        return content, ["Groq DeepSeek API Response", "AR6 Synthesis Report FAISS Data"]
                    except Exception as e:
                        print(f"DeepSeek error: {str(e)}")
                        error_msg = f"DeepSeek API call failed: {str(e)}"
                        return self.get_mock_response(user_message, report_focus)
                elif model == 'llama' and self.groq_client_llama:
                    try:
                        response = self.groq_client_llama.invoke(prompt_messages)
                        content = response.content
                        return content, ["Groq Llama API Response", "AR6 Synthesis Report FAISS Data"]
                    except Exception as e:
                        print(f"Llama error: {str(e)}")
                        error_msg = f"Llama API call failed: {str(e)}"
                        return self.get_mock_response(user_message, report_focus)
            except Exception as e:
                error_msg = f"FAISS Retrieval Error: {str(e)}\nFalling back to knowledge base."
                print(error_msg)
                content, sources = self.get_mock_response(user_message, report_focus)
                return f"{error_msg}\n\n{content}", sources + ["FAISS Error Fallback"]

        try:
            # Handle Groq models without FAISS
            if model == 'deepseek' and self.groq_client_deepseek:
                # Use system/user message format for consistency
                prompt_messages = [
                    {"role": "system", "content": deepseek_prompt},
                    {"role": "user", "content": messages[-1]['content']}
                ]
                print(f"Debug: Prompt messages for DeepSeek (No FAISS): {prompt_messages}")  # Debug: Log prompt
                try:
                    response = self.groq_client_deepseek.invoke(prompt_messages)
                    print(f"Debug: Raw response for DeepSeek (No FAISS): {response}")  # Debug: raw response
                    content = await self.clean_response(response.content if hasattr(response, 'content') else str(response))
                    if not content or not content.strip():
                        print("DeepSeek returned empty or invalid content after cleaning (No FAISS). Falling back to mock response.")
                        return self.get_mock_response(messages[-1]['content'], report_focus)
                    return content, ["Groq DeepSeek API Response"]
                except Exception as e:
                    print(f"DeepSeek error (No FAISS): {str(e)}")
                    error_msg = f"DeepSeek API call failed: {str(e)}"
                    return self.get_mock_response(messages[-1]['content'], report_focus)
            elif model == 'llama' and self.groq_client_llama:
                conversation_text = "\n".join([f"{msg['role']}: {msg['content']}" for msg in messages])
                full_prompt = f"{system_prompt}\n\nConversation:\n{conversation_text}"
                response = self.groq_client_llama.invoke(full_prompt)
                content = response.content
                return content, ["Groq Llama API Response"]
            elif model.startswith('gpt') and self.openai_client:
                response = await self.openai_client.chat.completions.create(
                    model="gpt-4-turbo-preview" if model == 'gpt-4' else "gpt-3.5-turbo",
                    messages=[{"role": "system", "content": system_prompt}] + messages,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                content = response.choices[0].message.content
                return content, ["OpenAI API Response"]
            elif model == 'claude-3' and self.anthropic_client:
                response = await self.anthropic_client.messages.create(
                    model="claude-3-sonnet-20240229",
                    system=system_prompt,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens
                )
                content = response.content[0].text
                return content, ["Anthropic Claude API Response"]
            elif model == 'gemini' and self.gemini_client:
                conversation_text = "\n".join([f"{msg['role']}: {msg['content']}" for msg in messages])
                full_prompt = f"{system_prompt}\n\nConversation:\n{conversation_text}"
                response = self.gemini_client.generate_content(
                    full_prompt,
                    generation_config={
                        'temperature': temperature,
                        'max_output_tokens': max_tokens
                    }
                )
                content = response.text
                return content, ["Google Gemini API Response"]
            else:
                user_message = messages[-1]['content'] if messages else ""
                return self.get_mock_response(user_message, report_focus)
        except Exception as e:
            error_msg = f"API Error: {str(e)}\n\nFalling back to demo mode with IPCC knowledge base."
            print(error_msg)
            user_message = messages[-1]['content'] if messages else ""
            content, sources = self.get_mock_response(user_message, report_focus)
            return f"{error_msg}\n\n{content}", sources + ["API Error Fallback"]

    async def process_message(self, message: str, history: List[Dict], model: str, temperature: float, max_tokens: int, report_focus: str) -> Tuple[List[Dict], str]:
        """Process user message and update chat history.

        Args:
            message: User input query string.
            history: List of chat history dictionaries.
            model: Selected LLM model key.
            temperature: Response creativity (0.0 to 1.0).
            max_tokens: Maximum response length.
            report_focus: Selected report key.

        Returns:
            Tuple of (updated history, empty string for UI).
        """
        if not message.strip():
            return history, ""  # Skip empty messages

        # Append user message to history
        history.append({"role": "user", "content": message})

        # Use history as messages for LLM API
        messages = history.copy()

        try:
            # Call LLM API and format response
            content, sources = await self.call_llm_api(messages, model, temperature, max_tokens, report_focus)
            formatted_response = self.format_response(content, sources, report_focus)
            history.append({"role": "assistant", "content": formatted_response})
        except Exception as e:
            # Handle errors gracefully
            error_response = f"‚ö†Ô∏è Error processing request: {str(e)}.\n\nPlease try again or check your API settings."
            print(f"Process error: {error_response}")
            history.append({"role": "assistant", "content": error_response})

        return history, ""

# Initialize the agent instance
agent = IPCCLLMAgent()

# Predefined quick prompts for the Gradio UI
quick_prompts = [
    "Summarize AR6 Synthesis Report key findings",
    "What urgent actions are needed by 2030?",
    "Explain carbon budgets in simple terms",
    "What are the main climate risks and impacts?",
    "Compare AR5 and AR6 projections",
    "What are the most effective mitigation strategies?",
    "How does climate change affect different regions?",
    "What adaptation measures are recommended?"
]

async def create_gradio_interface():
    """Create Gradio interface for the IPCC LLM Agent.

    Returns:
        Gradio Blocks interface object.
    """
    with gr.Blocks(
        title="üåç IPCC Climate Reports LLM Agent",
        theme=gr.themes.Soft(),
        css="""
        .gradio-container {
            max-width: 1200px !important;
        }
        .chat-message {
            padding: 10px;
            margin: 5px 0;
            border-radius: 10px;
        }
        """
    ) as interface:
        # Header with project description
        gr.HTML("""
        <div style='text-align: center; padding: 20px; background: linear-gradient(135deg, #667eea, #764ba2); color: white; border-radius: 10px;'>
            <h1>üåç IPCC Climate Reports LLM Agent</h1>
            <p style='font-size: 18px;'>AI-Powered Analysis of Climate Science Reports</p>
            <p style='font-size: 14px; opacity: 0.8;'>Google Colab Compatible ‚Ä¢ Multi-LLM Support ‚Ä¢ IPCC AR4/AR5/AR6 ‚Ä¢ FAISS Integration</p>
        </div>
        """)

        with gr.Row():
            with gr.Column(scale=3):  # Main chat area
                # Chatbot component for conversation display
                chatbot = gr.Chatbot(
                    value=[],
                    height=600,
                    show_label=False,
                    container=True,
                    type="messages"
                )
                with gr.Row():
                    # Text input for user queries
                    msg = gr.Textbox(
                        placeholder="Ask about IPCC climate reports, projections, or policies...",
                        label="Your Question",
                        scale=4,
                        lines=2
                    )
                    # Send button for query submission
                    send_btn = gr.Button("Send üöÄ", scale=1, variant="primary")

                # Clear chat history button
                clear_btn = gr.Button("Clear Chat üóëÔ∏è", scale=1, variant="secondary")
                clear_btn.click(lambda: [], outputs=[chatbot])

                # Quick prompt buttons for common queries
                gr.HTML("<h3>üí° Quick Prompts</h3>")
                with gr.Row():
                    for i in range(0, len(quick_prompts), 2):
                        with gr.Column():
                            if i < len(quick_prompts):
                                gr.Button(
                                    quick_prompts[i],
                                    size="sm"
                                ).click(
                                    lambda x=quick_prompts[i]: x,
                                    outputs=msg
                                )
                            if i+1 < len(quick_prompts):
                                gr.Button(
                                    quick_prompts[i+1],
                                    size="sm"
                                ).click(
                                    lambda x=quick_prompts[i+1]: x,
                                    outputs=msg
                                )

            with gr.Column(scale=1):  # Configuration sidebar
                gr.HTML("<h3>üîß Configuration</h3>")
                # Dropdown for selecting LLM model
                model_choice = gr.Dropdown(
                    choices=list(agent.llm_models.keys()),
                    value="mock",
                    label="AI Model",
                    info="Select preferred LLM (DeepSeek and Llama use same Groq API key)"
                )
                # Dropdown for selecting IPCC report focus
                report_focus = gr.Dropdown(
                    choices=['all', 'ar6_syr', 'ar6', 'ar5', 'special', 'ar4'],
                    value="all",
                    label="Report Focus",
                    info="Focus on specific IPCC reports (AR6 Synthesis uses FAISS)"
                )
                # Slider for LLM temperature
                temperature = gr.Slider(
                    minimum=0.0,
                    maximum=1.0,
                    value=0.7,
                    step=0.1,
                    label="Temperature",
                    info="Response creativity (0=focused, 1=creative)"
                )
                # Slider for maximum response length
                max_tokens = gr.Slider(
                    minimum=100,
                    maximum=2000,
                    value=1000,
                    step=100,
                    label="Max Tokens",
                    info="Response length limit"
                )
                # Instructions for API keys
                gr.HTML("<h3>üîë API Keys</h3>")
                gr.HTML("""
                <p style='font-size: 12px;'>
                Set API keys in Colab Secrets or environment variables:<br>
                ‚Ä¢ groq_api_key2 (Colab Secrets) for Groq (DeepSeek, Llama)<br>
                ‚Ä¢ OPENAI_API_KEY, ANTHROPIC_API_KEY, GEMINI_API_KEY (optional)<br>
                Use 'Mock AI' for demo mode.
                </p>
                """)
                # Display model and FAISS status
                gr.HTML("<h3>üìä Status</h3>")
                status_html = f"""
                <div style='font-size:12px;'>
                {'‚úÖ' if GROQ_AVAILABLE and (agent.groq_client_llama or agent.groq_client_deepseek) else '‚ùå'} Groq: {'Available' if (agent.groq_client_llama or agent.groq_client_deepseek) else 'No Key'}<br>
                {'‚úÖ' if OPENAI_AVAILABLE and agent.openai_client else '‚ùå'} OpenAI: {'Available' if agent.openai_client else 'No Key'}<br>
                {'‚úÖ' if ANTHROPIC_AVAILABLE and agent.anthropic_client else '‚ùå'} Anthropic: {'Available' if agent.anthropic_client else 'No Key'}<br>
                {'‚úÖ' if GEMINI_AVAILABLE and agent.gemini_client else '‚ùå'} Gemini: {'Available' if agent.gemini_client else 'No Key'}<br>
                {'‚úÖ' if FAISS_AVAILABLE and agent.faiss_retriever else '‚ùå'} FAISS: {'Available' if agent.faiss_retriever else 'Not Loaded'}<br>
                ‚úÖ Mock AI: Always Available
                </div>
                """
                gr.HTML(status_html)

        # Define response handler for send button and text submission
        async def respond(message, history, model, temp, tokens, report):
            return await agent.process_message(message, history, model, temp, tokens, report)

        # Bind send button to response handler
        send_btn.click(
            respond,
            inputs=[msg, chatbot, model_choice, temperature, max_tokens, report_focus],
            outputs=[chatbot, msg]
        )
        # Bind enter key to response handler
        msg.submit(
            respond,
            inputs=[msg, chatbot, model_choice, temperature, max_tokens, report_focus],
            outputs=[chatbot, msg]
        )

        # Footer with project credits
        gr.HTML("""
        <div style='text-align: center; padding: 10px; margin-top: 20px; border-top: 1px solid #ddd;'>
            <p>üöñ IPCC Climate Reports LLM Agent - Powered by AI for Climate Action</p>
            <p style='font-size:12px;'>AI-generated summaries of IPCC reports for educational purposes.</p>
        </div>
        """)

    return interface

async def main():
    """Main function to execute the IPCC LLM Agent in Google Colab."""
    print("üåç Initializing IPCC Climate Reports LLM Agent...")
    print("üìñ Loading IPCC knowledge base...")
    print("üîç Setting up model configurations...")
    from google.colab import drive
    drive.mount('/content/drive')  # Mount Google Drive for FAISS files
    interface = await create_gradio_interface()  # Create Gradio UI
    print("üöÄ Launching Gradio interface...")
    print("üí° Tip: Use 'Mock AI' for demo mode, or configure API key for Groq-hosted models")
    # Launch Gradio interface with public URL
    interface.launch(
        share=True,
        height=800,
        show_error=True,
        quiet=False
    )

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())  # Run the async main function



